{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7b33e08-120b-4a92-9d2d-b7600f8045de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "from matplotlib import pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a11d5fc-d2c7-4f2f-9460-427b19a5e2cb",
   "metadata": {},
   "source": [
    "# ------------------------ ***Curse Of Dimensionality***---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae5f147-3711-44de-ad1c-a2b09c5f3f54",
   "metadata": {},
   "source": [
    "<h3 style=\"color: red;\">Definition:</h3>\n",
    "<p>\n",
    "  The curse of dimensionality refers to the various problems that arise when analyzing\n",
    "  and organizing data in high-dimensional spaces that do not occur in low-dimensional settings.\n",
    "</p>\n",
    "\n",
    "<p>In simple terms, as the number of features (dimensions) increases:</p>\n",
    "<ul>\n",
    "  <li>Data becomes sparse.</li>\n",
    "  <li>Distance measures become less meaningful.</li>\n",
    "  <li>Models overfit more easily.</li>\n",
    "  <li>Computation becomes more expensive.</li>\n",
    "</ul>\n",
    "\n",
    "<h3 style=\"color: blue;\">Why is it a problem?</h3>\n",
    "<ul>\n",
    "  <li>\n",
    "    <strong>Sparsity:</strong> In higher dimensions, data points are spread thinly.\n",
    "    The more dimensions you add, the more data you need to maintain the same density.\n",
    "  </li>\n",
    "  <li>\n",
    "    <strong>Distance Metrics Fail:</strong> Many ML algorithms (e.g., k-NN, clustering) rely on distance.\n",
    "    But in high dimensions, distances between all points tend to become similar.\n",
    "  </li>\n",
    "  <li>\n",
    "    <strong>Overfitting:</strong> More features can lead to models fitting noise rather than signal.\n",
    "  </li>\n",
    "  <li>\n",
    "    <strong>Increased Computational Cost:</strong> Algorithms take longer and require more memory.\n",
    "  </li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bde45ea-8670-404d-b25f-f716c9e9e6d5",
   "metadata": {},
   "source": [
    "<h3 style=\"color: red;\">‚úÇÔ∏è Dimensionality Reduction</h3>\n",
    "\n",
    "<h4 style=\"color: darkorange;\">Definition:</h4>\n",
    "<p>\n",
    "  Dimensionality reduction is the process of reducing the number of random variables (features) under consideration\n",
    "  by obtaining a set of principal features.\n",
    "</p>\n",
    "\n",
    "<h4 style=\"color: blue;\">üß∞ Why do we need it?</h4>\n",
    "<ul>\n",
    "  <li>Mitigate the curse of dimensionality</li>\n",
    "  <li>Reduce overfitting</li>\n",
    "  <li>Improve model performance</li>\n",
    "  <li>Decrease training time</li>\n",
    "  <li>Improve visualization</li>\n",
    "</ul>\n",
    "\n",
    "<h4 style=\"color: green;\">‚úÖ Types of Dimensionality Reduction:</h4>\n",
    "\n",
    "<strong>1. Feature Selection (choose a subset of existing features):</strong>\n",
    "<ul>\n",
    "  <li>Filter methods (e.g., correlation, mutual information)</li>\n",
    "  <li>Wrapper methods (e.g., recursive feature elimination)</li>\n",
    "  <li>Embedded methods (e.g., LASSO)</li>\n",
    "</ul>\n",
    "\n",
    "<strong>2. Feature Extraction (create new features):</strong>\n",
    "<ul>\n",
    "  <li>PCA (Principal Component Analysis) ‚Äì projects data to directions of maximum variance.</li>\n",
    "  <li>t-SNE ‚Äì good for visualization in 2D/3D.</li>\n",
    "  <li>Autoencoders ‚Äì neural networks that compress and reconstruct data.</li>\n",
    "  <li>LDA (Linear Discriminant Analysis) ‚Äì projects data to maximize class separability.</li>\n",
    "</ul>\n",
    "\n",
    "<h4 style=\"color: purple;\">üìä Example of PCA:</h4>\n",
    "<p>\n",
    "  You have 4 features: height, weight, age, income.<br>\n",
    "  But age and income might be strongly correlated.<br>\n",
    "  PCA can combine them into 1 principal component that explains most of their joint variance.<br>\n",
    "  So, instead of training on 4 features, you train on 2 or 3 principal components that carry most of the information.\n",
    "</p>\n",
    "\n",
    "<h4 style=\"color: teal;\">üîç Real-world Analogy:</h4>\n",
    "<p>\n",
    "  Imagine you are viewing a 3D object (e.g., a sculpture), but you're only allowed to take a photo from one angle.\n",
    "  You choose the best angle that captures the most important features of the sculpture.<br>\n",
    "  That‚Äôs what dimensionality reduction does ‚Äî find the best \"view\" of your data in fewer dimensions.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6636fc3-1535-4ca3-b25e-19ddc6284269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00da8c2-daae-4881-968b-c5f3c853a2e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
