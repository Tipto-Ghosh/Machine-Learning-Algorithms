get_ipython().run_line_magic("config", " IPCompleter.use_jedi = False")
get_ipython().run_line_magic("config", " Completer.evaluation = 'limited'")
import warnings
warnings.filterwarnings('ignore')


import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt 
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score , accuracy_score


# Read the Data 
df = pd.read_csv('data.csv')
df.head()


X = df.drop(columns = ['target'])
y = df['target']


X_train , X_test , y_train , y_test = train_test_split(X , y , test_size = 0.2 , random_state = 2)











sns.pairplot(df , x_vars = ['feature1' , 'feature2' , 'feature3'] , y_vars = 'target' , kind = 'scatter')
plt.show()














# Using Correlation Metrices
df.corr()


# Find the VIF: if vif is arround 1 then no multocolinearity. If vif is around 5 then has multocolinearity
from statsmodels.stats.outliers_influence import variance_inflation_factor

vif = []
for i in range(X_train.shape[1]):
    vif.append(variance_inflation_factor(X_train , i))


pd.DataFrame({'vif' : vif} , index = df.columns[0 : 3]).T 


vif 











# to check this hypothesis we have to predict and find the errors
lr = LinearRegression()
lr.fit(X_train , y_train)


y_pred = lr.predict(X_test)


errors = (y_test - y_pred)



# errors of LinearRegression should be a Normal-Distribution
sns.histplot(errors , kde = True)

plt.xlabel('Errors')
plt.ylabel('Density')
plt.show()


# Also we can use QQ plot to test residual(errors) are Normal or not 
import scipy as sp 

fig , ax = plt.subplots(figsize = (6 , 4))

sp.stats.probplot(errors , plot = ax , fit = True)

plt.show()








plt.scatter(y_pred , errors)
plt.xlabel('y_pred')
plt.ylabel('residuals')
plt.show()








plt.plot(errors)
plt.show()



